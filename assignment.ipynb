{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/Input.csv')[['URL_ID','URL']]  \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('URL_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Fetched Successfully 0\n",
      "File Downloaded Successfully\n"
     ]
    }
   ],
   "source": [
    "#extracting text from all the url\n",
    "from importlib.metadata import files\n",
    "\n",
    "URL_ID=114\n",
    "for i in range(0,1):\n",
    "   URL = 'https://insights.blackcoffer.com/challenges-and-opportunities-of-big-data-in-healthcare/'\n",
    "   #j=df.iloc[i].values\n",
    "   \n",
    "   headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}#giving user access\n",
    "   page=requests.get(URL,headers=headers)#loading text in url\n",
    "   if(page.status_code==200):\n",
    "      print('Data Fetched Successfully',i)\n",
    "   soup=BeautifulSoup(page.content,'html.parser')#parsing url text\n",
    "   content=soup.findAll(attrs={'class':'td-post-content'})#extracting only text part\n",
    "   content=content[0].text.replace('\\xa0',\"  \").replace('\\n',\"  \")#replace end line symbol with space \n",
    "   title=soup.findAll(attrs={'class':'entry-title'})#extracting title of website\n",
    "   title=title[16].text.replace('\\n',\"  \").replace('/',\" \")\n",
    "   text=title+ '.' +content#merging title and content text\n",
    "   text=np.array(text)#converting to array form\n",
    "   text.reshape(1,-1)#changing shape to 1d \n",
    "   df1=pd.Series(text)#creating series data frame\n",
    "   b=str(URL)+\".\"+'txt'#name of the text file\n",
    "   with open('114.txt', 'a') as f:#creating text file \n",
    "    df1.to_csv(f, line_terminator=',', index=False, header=False)\n",
    "   #files.download(b)#downloading text file\n",
    "   print('File Downloaded Successfully')\n",
    "   #url_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing each extracted files\n",
    "text=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/114.txt\",header=None,encoding=\"ISO-8859-1\",sep='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 136.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#information of dataframe\n",
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting type\n",
    "text=text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text to sentence\n",
    "import re\n",
    "a=text[0].str.split('([\\.]\\s)',expand=False)#splitting text on '.'\n",
    "b=a.explode()#converting to rows\n",
    "b=pd.DataFrame(b)#creating data frame\n",
    "b.columns=['abc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing . char from each rows\n",
    "def abcd(x):    \n",
    "    nopunc =[char for char in x if char != '.']\n",
    "    return ''.join(nopunc)\n",
    "b['abc']=b['abc'].apply(abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing epmty space with null values\n",
    "c=b.replace('',np.nan,regex=True)\n",
    "c=c.mask(c==\" \")\n",
    "c=c.dropna()\n",
    "c.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Data  To begin with I shall first like to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big data is simply data, but with a huge size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data that is not just voluminous but also grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Such data sets are so large and complex that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Scarce resources like  data scientists are har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>They are easily  poached by competitors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>A good and efficient compensation strategy, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>In a nutshell, we can conclude that while bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Blackcoffer Insights 10 | Subhasmita Dey, Xav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  abc\n",
       "0   Challenges and Opportunities of Big Data in He...\n",
       "1    Big Data  To begin with I shall first like to...\n",
       "2       Big data is simply data, but with a huge size\n",
       "3   Data that is not just voluminous but also grow...\n",
       "4   Such data sets are so large and complex that i...\n",
       "..                                                ...\n",
       "61  Scarce resources like  data scientists are har...\n",
       "62            They are easily  poached by competitors\n",
       "63  A good and efficient compensation strategy, co...\n",
       "64    In a nutshell, we can conclude that while bi...\n",
       "65   Blackcoffer Insights 10 | Subhasmita Dey, Xav...\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 5172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nltk library and stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5174,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=[punc for punc in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 5175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stop words files that are provided\n",
    "StopWords_Auditor=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_Auditor.txt\",header=None)\n",
    "StopWords_Currencies=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_Currencies.txt\",header=None,encoding=\"ISO-8859-1\",sep=\"\\r\")\n",
    "StopWords_DatesandNumbers=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt\",header=None)\n",
    "StopWords_Generic=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_Generic.txt\",header=None)\n",
    "StopWords_GenericLong=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_GenericLong.txt\",header=None)\n",
    "StopWords_Geographic=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_Geographic.txt\",header=None)\n",
    "StopWords_Names=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/StopWords/StopWords_Names.txt\",header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating func for removing stop words\n",
    "def text_process(text):\n",
    "    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','’','?']]\n",
    "    nopunc=''.join(nopunc)\n",
    "    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])\n",
    "    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])\n",
    "    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])\n",
    "    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])\n",
    "    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])\n",
    "    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])\n",
    "    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying func for each row\n",
    "c['abc']=c['abc'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Challenges and Opportunities of Big Data in He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Data To begin with I shall first like to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big data is simply data but with a huge size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data that is not just voluminous but also grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Such data sets are so large and complex that i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Scarce resources like data scientists are hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>They are easily poached by competitors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>A good and efficient compensation strategy con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>In a nutshell we can conclude that while big d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Blackcoffer Insights 10 | Subhasmita Dey Xavie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  abc\n",
       "0   Challenges and Opportunities of Big Data in He...\n",
       "1   Big Data To begin with I shall first like to e...\n",
       "2        Big data is simply data but with a huge size\n",
       "3   Data that is not just voluminous but also grow...\n",
       "4   Such data sets are so large and complex that i...\n",
       "..                                                ...\n",
       "61  Scarce resources like data scientists are hard...\n",
       "62             They are easily poached by competitors\n",
       "63  A good and efficient compensation strategy con...\n",
       "64  In a nutshell we can conclude that while big d...\n",
       "65  Blackcoffer Insights 10 | Subhasmita Dey Xavie...\n",
       "\n",
       "[66 rows x 1 columns]"
      ]
     },
     "execution_count": 5179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing master Dictionary\n",
    "positive=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/MasterDictionary/positive-words.txt\",header=None)\n",
    "negative=pd.read_csv(\"C:/Users/cw/OneDrive/Desktop/Blackcoffer/20211030 Test Assignment/MasterDictionary/negative-words.txt\",header=None,encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5181,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive.columns=['abc']\n",
    "negative.columns=['abc']\n",
    "positive['abc']=positive['abc'].astype(str)\n",
    "negative['abc']=negative['abc'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive and negative dictionary without stopwords\n",
    "positive['abc']=positive['abc'].apply(text_process)\n",
    "negative['abc']=negative['abc'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive list\n",
    "length=positive.shape[0]\n",
    "post=[]\n",
    "for i in range(0,length):\n",
    "   nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']\n",
    "   nopunc=''.join(nopunc)\n",
    "\n",
    "   post.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative list\n",
    "length=negative.shape[0]\n",
    "neg=[]\n",
    "for i in range(0,length):\n",
    "  nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']\n",
    "  nopunc=''.join(nopunc)\n",
    "  neg.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tokenize library\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5186,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list=[]\n",
    "length=c.shape[0]\n",
    "for i in range(0,length):\n",
    "  txt=' '.join([word for word in c.iloc[i]])\n",
    "  txt_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization of text\n",
    "tokenize_text=[]\n",
    "for i in txt_list:\n",
    "  \n",
    "  tokenize_text+=(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Challenges', 'and', 'Opportunities', 'of', 'Big', 'Data', 'in', 'Healthcare', 'Big', 'Data', 'To', 'begin', 'with', 'I', 'shall', 'first', 'like', 'to', 'explain', 'what', 'big', 'data', 'is', 'and', 'why', 'it', 'has', 'become', 'so', 'important', 'in', 'our', 'lives', 'Big', 'data', 'is', 'simply', 'data', 'but', 'with', 'a', 'huge', 'size', 'Data', 'that', 'is', 'not', 'just', 'voluminous', 'but', 'also', 'growing', 'exponentially', 'with', 'time', 'Such', 'data', 'sets', 'are', 'so', 'large', 'and', 'complex', 'that', 'it', 'is', 'not', 'possible', 'to', 'store', 'or', 'process', 'them', 'using', 'traditional', 'data', 'management', 'tools', 'So', 'how', 'huge', 'can', 'this', 'data', 'be', 'Social', 'media', 'sites', 'like', 'Facebook', 'generate', 'more', 'than', '500', 'terabytes', 'of', 'new', 'data', 'every', 'day', 'in', 'the', 'form', 'of', 'photos', 'video', 'uploads', 'text', 'messages', 'etc', 'A', 'single', 'Jet', 'engine', 'can', 'generate', 'more', 'than', '10', 'terabytes', 'of', 'data', 'in', '30', 'minutes', 'of', 'flight', 'time', 'With', 'many', 'thousand', 'flights', 'per', 'day', 'generation', 'of', 'data', 'reaches', 'up', 'to', 'many', 'petabytes', 'Data', 'contained', 'in', 'these', 'sets', 'are', 'not', 'always', 'structured', 'It', 'can', 'be', 'semi-structured', 'or', 'even', 'unstructured', 'When', 'such', 'is', 'the', 'size', 'and', 'dimension', 'of', 'data', 'we', 'can', 'well', 'imagine', 'how', 'complicated', 'it', 'must', 'be', 'to', 'process', 'and', 'analyze', 'it', 'Therefore', 'modern', 'methods', 'of', 'data', 'analysis', 'are', 'being', 'developed', 'and', 'used', 'to', 'process', 'the', 'information', 'contained', 'in', 'big', 'data', 'sets', 'Opportunities', 'Now', 'coming', 'to', 'the', 'opportunities', 'that', 'Big', 'Data', 'provides', 'they', 'are', 'not', 'just', 'limited', 'to', 'healthcare', 'Big', 'data', 'analysis', 'is', 'now', 'a', 'disruptive', 'technology', 'which', 'has', 'intervened', 'into', 'numerous', 'fields', 'and', 'proved', 'its', 'worth', 'Healthcare', 'is', 'no', 'exception', 'Big', 'data', 'has', 'the', 'potential', 'to', 'change', 'the', 'entire', 'dynamics', 'of', 'the', 'healthcare', 'industry', 'and', 'improve', 'the', 'quality', 'of', 'life', 'of', 'people', 'Healthcare', 'industry', 'is', 'a', 'very', 'large', 'and', 'complicated', 'system', 'It', 'involves', 'a', 'lot', 'of', 'risks', 'and', 'always', 'demands', 'better', 'care', 'However', 'when', 'a', 'large', 'number', 'of', 'patients', 'seek', 'emergency', 'care', 'the', 'complications', 'as', 'well', 'as', 'the', 'cost', 'rise', 'exponentially', 'In', 'India', 'many', 'times', 'we', 'even', 'lack', 'sufficient', 'infrastructure', 'to', 'support', 'the', 'patients', 'there', 'is', 'a', 'deficit', 'of', 'beds', 'as', 'well', 'as', 'doctors', 'to', 'provide', 'treatment', 'to', 'the', 'patients', 'For', 'instance', 'when', 'epidemics', 'break', 'out', 'and', 'lives', 'are', 'lost', 'at', 'a', 'very', 'alarming', 'rate', 'we', 'can', 'easily', 'gauge', 'our', 'helplessness', 'However', 'the', 'scenario', 'is', 'certainly', 'improving', 'now', 'With', 'the', 'advent', 'of', 'digitization', 'into', 'the', 'healthcare', 'system', 'healthcare', 'providers', 'or', 'practitioners', 'are', 'now', 'having', 'access', 'to', 'a', 'huge', 'amount', 'of', 'patient', 'health', 'data', 'This', 'healthcare', 'big', 'data', 'can', 'be', 'processed', 'and', 'analyzed', 'to', 'identify', 'patient', 'patterns', 'more', 'quickly', 'and', 'effectively', 'The', 'information', 'obtained', 'can', 'be', 'extremely', 'useful', 'to', 'figure', 'out', 'chronic', 'health', 'issues', 'and', 'provide', 'preventive', 'treatment', 'plans', 'well', 'beforehand', 'so', 'as', 'to', 'curb', 'that', 'disease', 'or', 'disorder', 'from', 'occurring', 'This', 'method', 'is', 'also', 'known', 'as', 'predictive', 'analysis', 'and', 'it', 'is', 'one', 'of', 'the', 'most', 'crucial', 'benefits', 'of', 'big', 'data', 'in', 'healthcare', 'This', 'technology', 'can', 'help', 'the', 'healthcare', 'industry', 'in', 'more', 'ways', 'than', 'we', 'can', 'infer', 'Healthcare', 'organizations', 'that', 'have', 'implemented', 'predictive', 'analysis', 'have', 'witnessed', 'a', 'reduction', 'in', 'ER', 'visits', 'by', 'providing', 'support', 'and', 'care', 'to', 'patients', 'and', 'decreasing', 'emergency', 'situations', 'Apart', 'from', 'reduced', 'ER', 'visits', 'and', 'timely', 'treatment', 'there', 'are', 'many', 'other', 'benefits', 'of', 'big', 'data', 'and', 'predictive', 'analysis', 'Patients', 'with', 'high-risk', 'life-threatening', 'issues', 'can', 'be', 'provided', 'with', 'more', 'customized', 'treatment', 'facilities', 'Due', 'to', 'lack', 'of', 'data', 'there', 'exists', 'a', 'lack', 'of', 'proper', 'planning', 'in', 'hospitals', 'Under', 'or', 'over-booking', 'of', 'staff', 'lack', 'of', 'medical', 'equipment', 'medicines', 'and', 'other', 'facilities', 'are', 'a', 'result', 'of', 'inefficient', 'budgeting', 'and', 'ill-management', 'of', 'finances', 'Using', 'predictive', 'analysis', 'these', 'problems', 'can', 'be', 'solved', 'which', 'will', 'lead', 'to', 'reduced', 'costs', 'and', 'efficient', 'management', 'of', 'finances', 'Better', 'staff', 'allocation', 'and', 'admission', 'rate', 'prediction', 'shall', 'facilitate', 'improvement', 'in', 'daily', 'operations', 'of', 'healthcare', 'organizations', 'By', 'using', 'big', 'data', 'the', 'effect', 'of', 'recency', 'bias', 'could', 'be', 'reduced', 'as', 'well', 'When', 'we', 'give', 'more', 'importance', 'to', 'recent', 'events', 'and', 'tend', 'to', 'ignore', 'the', 'effect', 'of', 'the', 'older', 'ones', 'it', 'may', 'lead', 'to', 'incorrect', 'decisions', 'This', 'is', 'known', 'as', 'recency', 'bias', 'Big', 'data', 'also', 'helps', 'in', 'preventing', 'fraudulent', 'activities', 'which', 'in', 'turn', 'prevents', 'losses', 'of', 'insurance', 'companies', 'Challenges', 'While', 'all', 'of', 'this', 'is', 'changing', 'the', 'healthcare', 'industry', 'for', 'the', 'better', 'it', 'is', 'not', 'that', 'easy', 'to', 'reap', 'the', 'benefits', 'of', 'big', 'data', 'There', 'are', 'a', 'whole', 'lot', 'of', 'challenges', 'and', 'vulnerabilities', 'attached', 'to', 'its', 'implementation', 'One', 'of', 'the', 'biggest', 'challenges', 'is', 'security', 'Healthcare', 'big', 'data', 'contains', 'the', 'personal', 'information', 'and', 'health', 'history', 'of', 'patients', 'Acts', 'of', 'hacking', 'cyber', 'theft', 'and', 'phishing', 'pose', 'a', 'serious', 'threat', 'to', 'these', 'databases', 'Such', 'data', 'could', 'be', 'stolen', 'and', 'sold', 'for', 'huge', 'sums', 'of', 'money', 'Protection', 'of', 'the', 'patients\\x92', 'privacy', 'hence', 'is', 'a', 'serious', 'challenge', 'to', 'big', 'data', 'implementation', 'Also', 'the', 'data', 'would', 'contain', 'external', 'data', 'apart', 'from', 'medical', 'information', 'The', 'organization', 'therefore', 'has', 'to', 'take', 'care', 'of', 'privacy', 'legal', 'compliances', 'and', 'government', 'policies', 'Privacy', 'and', 'security', 'of', 'patients', 'have', 'to', 'be', 'given', 'utmost', 'importance', 'and', 'no', 'breach', 'of', 'any', 'kind', 'can', 'be', 'permitted', 'The', 'next', 'challenge', 'is', 'data', 'classification', 'and', 'modeling', 'The', 'size', 'of', 'the', 'data', 'is', 'massive', 'and', 'it', 'is', 'less', 'structured', 'and', 'heterogeneous', 'Classifying', 'such', 'massive', 'data', 'to', 'identify', 'relevant', 'information', 'is', 'a', 'big', 'challenge', 'Modeling', 'of', 'such', 'unstructured', 'data', 'is', 'equally', 'difficult', 'Storage', 'and', 'retrieval', 'is', 'another', 'major', 'challenge', 'Huge', 'cloud', 'servers', 'with', 'sufficient', 'space', 'are', 'required', 'to', 'store', 'such', 'voluminous', 'data', 'Also', 'the', 'speed', 'should', 'be', 'high', 'so', 'that', 'uploading', 'of', 'data', 'can', 'be', 'done', 'hassle-free', 'The', 'way', 'storage', 'is', 'a', 'challenge', 'retrieval', 'also', 'is', 'a', 'matter', 'of', 'concern', 'Integrating', 'the', 'data', 'and', 'getting', 'all', 'relevant', 'systems', 'to', 'link', 'each', 'other', 'is', 'a', 'tough', 'job', 'The', 'next', 'challenge', 'is', 'a', 'major', 'one', 'in', 'my', 'opinion', 'Finding', 'the', 'right', 'talent', 'who', 'own', 'the', 'expertise', 'to', 'implement', 'this', 'modern', 'technology', 'is', 'an', 'arduous', 'task', 'Shortage', 'of', 'required', 'talent', 'is', 'a', 'crisis', 'in', 'the', 'market', 'today', 'Even', 'after', 'hiring', 'the', 'right', 'talent', 'it', 'is', 'a', 'challenge', 'to', 'retain', 'them', 'Scarce', 'resources', 'like', 'data', 'scientists', 'are', 'hard', 'to', 'find', 'and', 'even', 'harder', 'to', 'retain', 'They', 'are', 'easily', 'poached', 'by', 'competitors', 'A', 'good', 'and', 'efficient', 'compensation', 'strategy', 'conducive', 'work', 'environment', 'high', 'incentives', 'opportunities', 'for', 'career', 'growth', 'and', 'development', 'can', 'be', 'some', 'of', 'the', 'ways', 'of', 'retaining', 'such', 'intellectual', 'talent', 'In', 'a', 'nutshell', 'we', 'can', 'conclude', 'that', 'while', 'big', 'data', 'is', 'a', 'disruptive', 'technology', 'which', 'will', 'bring', 'about', 'landmark', 'changes', 'in', 'healthcare', 'dynamics', 'the', 'challenges', 'and', 'vulnerabilities', 'need', 'to', 'be', 'addressed', 'with', 'the', 'utmost', 'care', 'and', 'sense', 'of', 'responsibility', 'Blackcoffer', 'Insights', '10', '|', 'Subhasmita', 'Dey', 'Xavier', 'School', 'of', 'Human', 'Resource', 'Management', 'XUB']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1054"
      ]
     },
     "execution_count": 5189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive score= 47\n"
     ]
    }
   ],
   "source": [
    "positive_score=0\n",
    "for i in tokenize_text:\n",
    "  if(i.lower() in post):\n",
    "    positive_score+=1\n",
    "print('positive score=', positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative score= 41\n"
     ]
    }
   ],
   "source": [
    "negative_score=0\n",
    "for i in tokenize_text:\n",
    "  if(i.lower() in neg):\n",
    "    negative_score+=1\n",
    "print('negative score=', negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity_score= 0.06818181740702481\n"
     ]
    }
   ],
   "source": [
    "#Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
    "print('polarity_score=', Polarity_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjectivity_score 0.00569259961509241\n"
     ]
    }
   ],
   "source": [
    "#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)\n",
    "print('subjectivity_score',subjectiivity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg sentence length= 95.54545454545455\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "  avg_length.append(len(c['abc'].iloc[i]))\n",
    "avg_senetence_length=sum(avg_length)/len(avg_length)\n",
    "print('avg sentence length=', avg_senetence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "  if(count>2):\n",
    "   complex_Word_Count+=1\n",
    "  count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentag of complex words=  0.2647058823529412\n"
     ]
    }
   ],
   "source": [
    "Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)\n",
    "print('percentag of complex words= ',Percentage_of_Complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fog index=  38.324064171123\n"
     ]
    }
   ],
   "source": [
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "Fog_Index = 0.4 * (avg_senetence_length + Percentage_of_Complex_words)\n",
    "print('fog index= ',Fog_Index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg no of words per sentence=  15.969696969696969\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "  a=[word.split( ) for word in c.iloc[i]]\n",
    "  avg_length.append(len(a[0]))\n",
    "  a=0\n",
    "#avg\n",
    "avg_no_of_words_per_sentence=sum(avg_length)/length\n",
    "print(\"avg no of words per sentence= \",avg_no_of_words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex words count= 279\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "  if(count>2):\n",
    "   complex_Word_Count+=1\n",
    "  count=0\n",
    "print('complex words count=',  complex_Word_Count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count=  1054\n"
     ]
    }
   ],
   "source": [
    "word_count=len(tokenize_text)\n",
    "print('word count= ', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syllable_per_word=  2065\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "  x=re.compile('[es|ed]$')\n",
    "  if x.match(i.lower()):\n",
    "   count+=0\n",
    "  else:\n",
    "    for j in i:\n",
    "      if(j.lower() in vowels ):\n",
    "        count+=1\n",
    "syllable_count=count\n",
    "print('syllable_per_word= ',syllable_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal pronouns=  8\n"
     ]
    }
   ],
   "source": [
    "pronouns=['i','we','my','ours','us' ]\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "  if i.lower() in pronouns:\n",
    "   count+=1\n",
    "personal_pronouns=count\n",
    "print('personal pronouns= ',personal_pronouns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg word=  5.045540796963947\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in tokenize_text:\n",
    "  for j in i:\n",
    "    count+=1\n",
    "avg_word_length=count/len(tokenize_text)\n",
    "print('avg word= ', avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5204,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'URL_ID':URL_ID,'URL':URL,'positive_score':positive_score,'negative_score':negative_score,'Polarity_Score':Polarity_Score,'subjectiivity_score':subjectiivity_score,'avg_senetence_length':avg_senetence_length,'Percentage_of_Complex_words':Percentage_of_Complex_words,'Fog_Index':Fog_Index,'avg_no_of_words_per_sentence':avg_no_of_words_per_sentence,'complex_Word_Count':complex_Word_Count,'word_count':word_count,'syllable_count':syllable_count,'personal_pronouns':personal_pronouns,'avg_word_length':avg_word_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cw\\AppData\\Local\\Temp\\ipykernel_2304\\2807884338.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  output=output.append(data,ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity_Score</th>\n",
       "      <th>subjectiivity_score</th>\n",
       "      <th>avg_senetence_length</th>\n",
       "      <th>Percentage_of_Complex_words</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>avg_no_of_words_per_sentence</th>\n",
       "      <th>complex_Word_Count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>personal_pronouns</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>78</td>\n",
       "      <td>34</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.024458</td>\n",
       "      <td>155.090909</td>\n",
       "      <td>0.352974</td>\n",
       "      <td>62.177553</td>\n",
       "      <td>23.337662</td>\n",
       "      <td>635</td>\n",
       "      <td>1799</td>\n",
       "      <td>3924</td>\n",
       "      <td>1</td>\n",
       "      <td>5.682046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>71</td>\n",
       "      <td>38</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>118.028571</td>\n",
       "      <td>0.239437</td>\n",
       "      <td>47.307203</td>\n",
       "      <td>20.285714</td>\n",
       "      <td>340</td>\n",
       "      <td>1420</td>\n",
       "      <td>2667</td>\n",
       "      <td>6</td>\n",
       "      <td>4.866901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>74</td>\n",
       "      <td>38</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>126.720930</td>\n",
       "      <td>0.332558</td>\n",
       "      <td>50.821395</td>\n",
       "      <td>19.883721</td>\n",
       "      <td>572</td>\n",
       "      <td>1720</td>\n",
       "      <td>3607</td>\n",
       "      <td>2</td>\n",
       "      <td>5.391860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>85</td>\n",
       "      <td>28</td>\n",
       "      <td>0.504425</td>\n",
       "      <td>0.034193</td>\n",
       "      <td>115.289157</td>\n",
       "      <td>0.235753</td>\n",
       "      <td>46.209964</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>393</td>\n",
       "      <td>1667</td>\n",
       "      <td>3127</td>\n",
       "      <td>18</td>\n",
       "      <td>4.794241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>143.743243</td>\n",
       "      <td>0.259091</td>\n",
       "      <td>57.600934</td>\n",
       "      <td>23.608108</td>\n",
       "      <td>456</td>\n",
       "      <td>1760</td>\n",
       "      <td>3375</td>\n",
       "      <td>16</td>\n",
       "      <td>5.093182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>110</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>111.730769</td>\n",
       "      <td>0.286492</td>\n",
       "      <td>44.806905</td>\n",
       "      <td>17.615385</td>\n",
       "      <td>263</td>\n",
       "      <td>918</td>\n",
       "      <td>1811</td>\n",
       "      <td>11</td>\n",
       "      <td>5.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>111</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.020138</td>\n",
       "      <td>159.316667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>63.840952</td>\n",
       "      <td>25.933333</td>\n",
       "      <td>454</td>\n",
       "      <td>1589</td>\n",
       "      <td>3047</td>\n",
       "      <td>18</td>\n",
       "      <td>5.074261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>112</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>32</td>\n",
       "      <td>47</td>\n",
       "      <td>-0.189873</td>\n",
       "      <td>-0.012876</td>\n",
       "      <td>103.485294</td>\n",
       "      <td>0.313305</td>\n",
       "      <td>41.519440</td>\n",
       "      <td>17.073529</td>\n",
       "      <td>365</td>\n",
       "      <td>1165</td>\n",
       "      <td>2300</td>\n",
       "      <td>3</td>\n",
       "      <td>5.102146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>113</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.043896</td>\n",
       "      <td>161.033333</td>\n",
       "      <td>0.355281</td>\n",
       "      <td>64.555446</td>\n",
       "      <td>24.066667</td>\n",
       "      <td>259</td>\n",
       "      <td>729</td>\n",
       "      <td>1566</td>\n",
       "      <td>0</td>\n",
       "      <td>5.677641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>114</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>47</td>\n",
       "      <td>41</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>95.545455</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>38.324064</td>\n",
       "      <td>15.969697</td>\n",
       "      <td>279</td>\n",
       "      <td>1054</td>\n",
       "      <td>2065</td>\n",
       "      <td>8</td>\n",
       "      <td>5.045541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0         1  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1         2  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2         3  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3         4  https://insights.blackcoffer.com/will-machine-...   \n",
       "4         5  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "99      110  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "100     111  https://insights.blackcoffer.com/the-future-of...   \n",
       "101     112  https://insights.blackcoffer.com/big-data-anal...   \n",
       "102     113  https://insights.blackcoffer.com/business-anal...   \n",
       "103     114  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     positive_score  negative_score  Polarity_Score  subjectiivity_score  \\\n",
       "0                78              34        0.392857             0.024458   \n",
       "1                71              38        0.302752             0.023239   \n",
       "2                74              38        0.321429             0.020930   \n",
       "3                85              28        0.504425             0.034193   \n",
       "4                68              27        0.431579             0.023295   \n",
       "..              ...             ...             ...                  ...   \n",
       "99               29              27        0.035714             0.002179   \n",
       "100              46              14        0.533333             0.020138   \n",
       "101              32              47       -0.189873            -0.012876   \n",
       "102              36               4        0.800000             0.043896   \n",
       "103              47              41        0.068182             0.005693   \n",
       "\n",
       "     avg_senetence_length  Percentage_of_Complex_words  Fog_Index  \\\n",
       "0              155.090909                     0.352974  62.177553   \n",
       "1              118.028571                     0.239437  47.307203   \n",
       "2              126.720930                     0.332558  50.821395   \n",
       "3              115.289157                     0.235753  46.209964   \n",
       "4              143.743243                     0.259091  57.600934   \n",
       "..                    ...                          ...        ...   \n",
       "99             111.730769                     0.286492  44.806905   \n",
       "100            159.316667                     0.285714  63.840952   \n",
       "101            103.485294                     0.313305  41.519440   \n",
       "102            161.033333                     0.355281  64.555446   \n",
       "103             95.545455                     0.264706  38.324064   \n",
       "\n",
       "     avg_no_of_words_per_sentence  complex_Word_Count  word_count  \\\n",
       "0                       23.337662                 635        1799   \n",
       "1                       20.285714                 340        1420   \n",
       "2                       19.883721                 572        1720   \n",
       "3                       20.000000                 393        1667   \n",
       "4                       23.608108                 456        1760   \n",
       "..                            ...                 ...         ...   \n",
       "99                      17.615385                 263         918   \n",
       "100                     25.933333                 454        1589   \n",
       "101                     17.073529                 365        1165   \n",
       "102                     24.066667                 259         729   \n",
       "103                     15.969697                 279        1054   \n",
       "\n",
       "     syllable_count  personal_pronouns  avg_word_length  \n",
       "0              3924                  1         5.682046  \n",
       "1              2667                  6         4.866901  \n",
       "2              3607                  2         5.391860  \n",
       "3              3127                 18         4.794241  \n",
       "4              3375                 16         5.093182  \n",
       "..              ...                ...              ...  \n",
       "99             1811                 11         5.387800  \n",
       "100            3047                 18         5.074261  \n",
       "101            2300                  3         5.102146  \n",
       "102            1566                  0         5.677641  \n",
       "103            2065                  8         5.045541  \n",
       "\n",
       "[104 rows x 15 columns]"
      ]
     },
     "execution_count": 5205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=pd.DataFrame(output)\n",
    "output=output.append(data,ignore_index=True)\n",
    "output.columns=['URL_ID','URL','positive_score','negative_score','Polarity_Score','subjectiivity_score','avg_senetence_length','Percentage_of_Complex_words','Fog_Index','avg_no_of_words_per_sentence','complex_Word_Count','word_count','syllable_count','personal_pronouns','avg_word_length']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output,columns=['URL_ID','URL','positive_score','negative_score','Polarity_Score','subjectiivity_score','avg_senetence_length','Percentage_of_Complex_words','Fog_Index','avg_no_of_words_per_sentence','complex_Word_Count','word_count','syllable_count','personal_pronouns','avg_word_length'])\n",
    "df.to_csv('assign.csv',mode='a',index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d424d5778208ab1d0bf245b8cf56d0add48382fc1b816c362415a689958fc521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
